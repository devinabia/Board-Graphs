"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ResultSet = void 0;
const client_common_1 = require("@clickhouse/client-common");
const utils_1 = require("./utils");
const NEWLINE = 0x0a;
class ResultSet {
    constructor(_stream, format, query_id, _response_headers) {
        Object.defineProperty(this, "_stream", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: _stream
        });
        Object.defineProperty(this, "format", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: format
        });
        Object.defineProperty(this, "query_id", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: query_id
        });
        Object.defineProperty(this, "response_headers", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "isAlreadyConsumed", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        this.response_headers =
            _response_headers !== undefined ? Object.freeze(_response_headers) : {};
    }
    /** See {@link BaseResultSet.text} */
    async text() {
        this.markAsConsumed();
        return (0, utils_1.getAsText)(this._stream);
    }
    /** See {@link BaseResultSet.json} */
    async json() {
        // JSONEachRow, etc.
        if ((0, client_common_1.isStreamableJSONFamily)(this.format)) {
            const result = [];
            const reader = this.stream().getReader();
            // eslint-disable-next-line no-constant-condition
            while (true) {
                const { done, value } = await reader.read();
                if (done) {
                    break;
                }
                for (const row of value) {
                    result.push(row.json());
                }
            }
            return result;
        }
        // JSON, JSONObjectEachRow, etc.
        if ((0, client_common_1.isNotStreamableJSONFamily)(this.format)) {
            const text = await (0, utils_1.getAsText)(this._stream);
            return JSON.parse(text);
        }
        // should not be called for CSV, etc.
        throw new Error(`Cannot decode ${this.format} as JSON`);
    }
    /** See {@link BaseResultSet.stream} */
    stream() {
        this.markAsConsumed();
        (0, client_common_1.validateStreamFormat)(this.format);
        let incompleteChunks = [];
        let totalIncompleteLength = 0;
        const decoder = new TextDecoder('utf-8');
        const transform = new TransformStream({
            start() {
                //
            },
            transform: (chunk, controller) => {
                if (chunk === null) {
                    controller.terminate();
                }
                const rows = [];
                let idx;
                let lastIdx = 0;
                do {
                    // an unescaped newline character denotes the end of a row
                    idx = chunk.indexOf(NEWLINE, lastIdx);
                    // there is no complete row in the rest of the current chunk
                    if (idx === -1) {
                        // to be processed during the next transform iteration
                        const incompleteChunk = chunk.slice(lastIdx);
                        incompleteChunks.push(incompleteChunk);
                        totalIncompleteLength += incompleteChunk.length;
                        // send the extracted rows to the consumer, if any
                        if (rows.length > 0) {
                            controller.enqueue(rows);
                        }
                    }
                    else {
                        let text;
                        if (incompleteChunks.length > 0) {
                            const completeRowBytes = new Uint8Array(totalIncompleteLength + idx);
                            // using the incomplete chunks from the previous iterations
                            let offset = 0;
                            incompleteChunks.forEach((incompleteChunk) => {
                                completeRowBytes.set(incompleteChunk, offset);
                                offset += incompleteChunk.length;
                            });
                            // finalize the row with the current chunk slice that ends with a newline
                            const finalChunk = chunk.slice(0, idx);
                            completeRowBytes.set(finalChunk, offset);
                            // reset the incomplete chunks
                            incompleteChunks = [];
                            totalIncompleteLength = 0;
                            text = decoder.decode(completeRowBytes);
                        }
                        else {
                            text = decoder.decode(chunk.slice(lastIdx, idx));
                        }
                        rows.push({
                            text,
                            json() {
                                return JSON.parse(text);
                            },
                        });
                        lastIdx = idx + 1; // skipping newline character
                    }
                } while (idx !== -1);
            },
        });
        const pipeline = this._stream.pipeThrough(transform, {
            preventClose: false,
            preventAbort: false,
            preventCancel: false,
        });
        return pipeline;
    }
    async close() {
        this.markAsConsumed();
        await this._stream.cancel();
    }
    markAsConsumed() {
        if (this.isAlreadyConsumed) {
            throw new Error(streamAlreadyConsumedMessage);
        }
        this.isAlreadyConsumed = true;
    }
}
exports.ResultSet = ResultSet;
const streamAlreadyConsumedMessage = 'Stream has been already consumed';
//# sourceMappingURL=result_set.js.map